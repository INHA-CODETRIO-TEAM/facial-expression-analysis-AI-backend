{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1hNsPSc8c02XpRU3KRBaQOzVK40I5QYgw","authorship_tag":"ABX9TyMWFcOSUNj5q5tQfdRgMbmf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Vision Transformer (ViT) Pytorch"],"metadata":{"id":"vin4BJ4aFfzT"}},{"cell_type":"markdown","source":["Patch Embedding"],"metadata":{"id":"Tl0VGflpFv9a"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5R2TPVHGeSl","executionInfo":{"status":"ok","timestamp":1700564492210,"user_tz":-540,"elapsed":6278,"user":{"displayName":"김성아","userId":"17267080986120858203"}},"outputId":"0d42285e-555c-4a97-e237-da73d9a7d385"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch import nn\n","from torch import Tensor\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from einops import rearrange, reduce, repeat\n","from einops.layers.torch import Rearrange, Reduce\n","from torchsummary import summary\n","import os"],"metadata":{"id":"kDTmf2PrFtBe","executionInfo":{"status":"ok","timestamp":1700564496814,"user_tz":-540,"elapsed":4606,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Q5QwK2Wd_gk7","executionInfo":{"status":"ok","timestamp":1700564496815,"user_tz":-540,"elapsed":9,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import random\n","from torch.nn import Module\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from functools import partial\n","from typing import Any, Callable, List, Optional, Type, Union\n","from torch import Tensor"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"I_0XtdkbvtKe","executionInfo":{"status":"ok","timestamp":1700564496815,"user_tz":-540,"elapsed":8,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["class CustomTrainDataset1(Dataset,Module):\n","    def __init__(self, folder_path, index_array, transform=None):\n","        self.folder_path = folder_path\n","        self.image_files=[os.path.join(folder_path,file) for file in os.listdir(folder_path)]\n","        self.image_paths=random.sample(self.image_files,7000)\n","        #self.image_paths = [os.path.join(folder_path, img) for img in os.listdir(folder_path)[:7000] if img.endswith('.jpg')]\n","        self.label = index_array\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image_name = os.path.basename(image_path)  # 이미지 파일의 이름을 가져오기\n","        image = Image.open(image_path).convert('RGB')  # 이미지를 열고 RGB 형식으로 변환\n","        if self.transform:\n","            image = self.transform(image)\n","        return image,self.label\n","\n","# 이미지 전처리를 위한 변환기 설정\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # 이미지 크기 조절\n","    transforms.ToTensor(),           # 이미지를 텐서로 변환\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n","])\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dnQwaZzGvufW","executionInfo":{"status":"ok","timestamp":1700564496815,"user_tz":-540,"elapsed":7,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["class CustomTrainDataset2(Dataset,Module):\n","    def __init__(self, folder_path, index_array, transform=None):\n","        self.folder_path = folder_path\n","        image_files=[os.path.join(folder_path,file) for file in os.listdir(folder_path)]\n","        self.image_paths=random.sample(image_files,3000)\n","        self.label = index_array\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image_name = os.path.basename(image_path)  # 이미지 파일의 이름을 가져오기\n","        image = Image.open(image_path).convert('RGB')  # 이미지를 열고 RGB 형식으로 변환\n","        if self.transform:\n","            image = self.transform(image)\n","        return image,self.label\n","\n","# 이미지 전처리를 위한 변환기 설정\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # 이미지 크기 조절\n","    transforms.ToTensor(),           # 이미지를 텐서로 변환\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n","])\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ae14TWlBv3W_","executionInfo":{"status":"ok","timestamp":1700564496815,"user_tz":-540,"elapsed":7,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["class CustomValidDataset(Dataset,Module):\n","    def __init__(self, folder_path, index_array, transform=None):\n","        self.folder_path = folder_path\n","        image_files=[os.path.join(folder_path,file) for file in os.listdir(folder_path)]\n","        self.image_paths=random.sample(image_files,1000)\n","        #self.image_paths = [os.path.join(folder_path, img) for img in os.listdir(folder_path)[:1000] if img.endswith('.jpg')]\n","        self.label = index_array\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image_name = os.path.basename(image_path)  # 이미지 파일의 이름을 가져오기\n","        image = Image.open(image_path).convert('RGB')  # 이미지를 열고 RGB 형식으로 변환\n","        if self.transform:\n","            image = self.transform(image)\n","        return image,self.label\n","\n","# 이미지 전처리를 위한 변환기 설정\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # 이미지 크기 조절\n","    transforms.ToTensor(),           # 이미지를 텐서로 변환\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n","])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NaC4byRNv88T","executionInfo":{"status":"ok","timestamp":1700564496815,"user_tz":-540,"elapsed":6,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["import pickle\n","\n","def save_data_loader(dataloader, file_path):\n","    with open(file_path, 'wb') as file:\n","        pickle.dump(dataloader, file)\n","\n","def load_data_loader(file_path):\n","    with open(file_path, 'rb') as file:\n","        dataloader = pickle.load(file)\n","    return dataloader"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gZoljU2Iv_ek","executionInfo":{"status":"ok","timestamp":1700564499365,"user_tz":-540,"elapsed":2556,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"outputs":[],"source":["# DataLoader 객체를 파일에 저장\n","#save_dataset(datasets, 'dataset.pkl')\n","\n","# 파일에서 DataLoader 객체 불러오기\n","train_loader = load_data_loader('/content/drive/MyDrive/종설/train_Dataloader.pkl')\n","valid_loader = load_data_loader('/content/drive/MyDrive/종설/valid_Dataloader.pkl')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1700564499366,"user":{"displayName":"김성아","userId":"17267080986120858203"},"user_tz":-540},"id":"ZATV1Y7IwB2E","outputId":"92ee1d94-d5d4-4600-f932-f90ced77d709"},"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","6000\n"]}],"source":["print(len(train_loader.dataset))\n","print(len(valid_loader.dataset))"]},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","  def __init__(self, in_channels:int=3, patch_size:int=16, emb_size:int=768, img_size:int=224):\n","    self.P = patch_size\n","    super().__init__()\n","    self.projection = nn.Sequential(\n","        nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n","        Rearrange('b e h w -> b (h w) e')\n","    )\n","    self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n","    self.positions = nn.Parameter(torch.randn((img_size//patch_size)**2+1,emb_size))\n","  def forward(self, x:Tensor)->Tensor:\n","    b,_,_,_ = x.shape\n","    x = self.projection(x)\n","    cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n","    x = torch.cat([cls_tokens, x], dim=1)\n","    x += self.positions\n","    return x"],"metadata":{"id":"FDJPNpucRN6P","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Multi head attention"],"metadata":{"id":"tJlW8qV1fway"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, emb_size:int=7689, num_heads:int=8, dropout:float=0):\n","    super().__init__()\n","    self.emb_size = emb_size\n","    self.num_heads = num_heads\n","    #Fuse the queries, keys, values in one matrix\n","    self.qkv = nn.Linear(emb_size, emb_size*3)\n","    self.att_drop = nn.Dropout(dropout)\n","    self.projection = nn.Linear(emb_size, emb_size)\n","\n","  def forward(self, x:Tensor, mask:Tensor = None) -> Tensor:\n","    #split keys, queries, and values in num_heads\n","    qkv = rearrange(self.qkv(x), 'b n (h d qkv) -> (qkv) b h n d',\n","                    h=self.num_heads, qkv=3)\n","    queries, keys, values = qkv[0],qkv[1],qkv[2]\n","\n","    #sum up over the last axis\n","    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n","    if mask is not None:\n","      fill_value = torch.finfo(torch.float32).min\n","      energy.mask_fill(~mask, fill_value)\n","\n","    scaling = self.emb_size**(1/2)\n","    att = F.softmax(energy, dim=-1)/scaling\n","    att = self.att_drop(att)\n","\n","    #sum up over the third axis\n","    out = torch.einsum('bhal, bhlv -> bhav',att, values)\n","    out = rearrange(out, 'b h n d -> b n (h d)')\n","    out = self.projection(out)\n","    return out"],"metadata":{"id":"CpxIhwpNqmrg","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":12,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Residual Block"],"metadata":{"id":"3WyTWJxDGDCG"}},{"cell_type":"code","source":["class ResidualAdd(nn.Module):\n","  def __init__(self,fn):\n","    super().__init__()\n","    self.fn = fn\n","  def forward(self, x, **kwargs):\n","    res = x\n","    x = self.fn(x, **kwargs)\n","    x += res\n","    return x"],"metadata":{"id":"A13irLn6GB6S","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":12,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Feed Forward MLP"],"metadata":{"id":"skHOdpEpJB40"}},{"cell_type":"code","source":["class FeedForwardBlock(nn.Sequential):\n","  def __init__(self, emb_size:int, expansion:int=4, drop_p:float=0.):\n","    super().__init__(\n","        nn.Linear(emb_size, expansion*emb_size),\n","        nn.GELU(),\n","        nn.Dropout(drop_p),\n","        nn.Linear(expansion*emb_size, emb_size)\n","    )"],"metadata":{"id":"Ib8OaDi7JBCJ","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":11,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Transformer Encoder Block"],"metadata":{"id":"7cZprwPSJ6dA"}},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Sequential):\n","  def __init__(self,\n","               emb_size:int=768,\n","               drop_p:float=0,\n","               forward_expansion:int=4,\n","               forward_drop_p:float=0.,\n","               **kwargs):\n","    super().__init__(\n","        ResidualAdd(nn.Sequential(\n","            nn.LayerNorm(emb_size),\n","            MultiHeadAttention(emb_size, **kwargs),\n","            nn.Dropout(drop_p)\n","        )),\n","        ResidualAdd(nn.Sequential(\n","            nn.LayerNorm(emb_size),\n","            FeedForwardBlock(\n","                emb_size, expansion=forward_expansion, drop_p=forward_drop_p\n","            )\n","        ))\n","    )"],"metadata":{"id":"vXIkBJFmJ4hy","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":11,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Building Block"],"metadata":{"id":"RMjTbBnkPwZR"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Sequential):\n","  def __init__(self, depth:int=12, **kwargs):\n","    super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"],"metadata":{"id":"ACRW4RDuPnwu","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":11,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Head"],"metadata":{"id":"n3ts09JPao6Z"}},{"cell_type":"code","source":["class ClassificationHead(nn.Sequential):\n","  def __init__(self, emb_size:int=768, n_classes: int=10):\n","    super().__init__(\n","        Reduce('b n e -> b e', reduction ='mean'),\n","        nn.LayerNorm(emb_size),\n","        nn.Linear(emb_size, n_classes)\n","    )"],"metadata":{"id":"ho8LOiHOVV4r","executionInfo":{"status":"ok","timestamp":1700564499366,"user_tz":-540,"elapsed":11,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Summary"],"metadata":{"id":"vAp6i3jabEIB"}},{"cell_type":"code","source":["class ViT(nn.Sequential):\n","  def __init__(self,\n","               in_channels: int=3,\n","               patch_size: int=16,\n","               emb_size: int=768,\n","               img_size:int=12,\n","               depth: int=12,\n","               n_classes: int=10,\n","               **kwargs):\n","    super().__init__(\n","        PatchEmbedding(in_channels,patch_size, emb_size, img_size),\n","        TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n","        ClassificationHead(emb_size, n_classes)\n","    )\n","\n","summary(ViT(), (3,224,224), device='cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pG4wX6gSbC6X","executionInfo":{"status":"ok","timestamp":1700564500673,"user_tz":-540,"elapsed":1317,"user":{"displayName":"김성아","userId":"17267080986120858203"}},"outputId":"069fa8fc-4817-4834-ddb3-0139d526f83c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 14, 14]         590,592\n","         Rearrange-2             [-1, 196, 768]               0\n","    PatchEmbedding-3             [-1, 197, 768]               0\n","         LayerNorm-4             [-1, 197, 768]           1,536\n","            Linear-5            [-1, 197, 2304]       1,771,776\n","           Dropout-6          [-1, 8, 197, 197]               0\n","            Linear-7             [-1, 197, 768]         590,592\n","MultiHeadAttention-8             [-1, 197, 768]               0\n","           Dropout-9             [-1, 197, 768]               0\n","      ResidualAdd-10             [-1, 197, 768]               0\n","        LayerNorm-11             [-1, 197, 768]           1,536\n","           Linear-12            [-1, 197, 3072]       2,362,368\n","             GELU-13            [-1, 197, 3072]               0\n","          Dropout-14            [-1, 197, 3072]               0\n","           Linear-15             [-1, 197, 768]       2,360,064\n","      ResidualAdd-16             [-1, 197, 768]               0\n","        LayerNorm-17             [-1, 197, 768]           1,536\n","           Linear-18            [-1, 197, 2304]       1,771,776\n","          Dropout-19          [-1, 8, 197, 197]               0\n","           Linear-20             [-1, 197, 768]         590,592\n","MultiHeadAttention-21             [-1, 197, 768]               0\n","          Dropout-22             [-1, 197, 768]               0\n","      ResidualAdd-23             [-1, 197, 768]               0\n","        LayerNorm-24             [-1, 197, 768]           1,536\n","           Linear-25            [-1, 197, 3072]       2,362,368\n","             GELU-26            [-1, 197, 3072]               0\n","          Dropout-27            [-1, 197, 3072]               0\n","           Linear-28             [-1, 197, 768]       2,360,064\n","      ResidualAdd-29             [-1, 197, 768]               0\n","        LayerNorm-30             [-1, 197, 768]           1,536\n","           Linear-31            [-1, 197, 2304]       1,771,776\n","          Dropout-32          [-1, 8, 197, 197]               0\n","           Linear-33             [-1, 197, 768]         590,592\n","MultiHeadAttention-34             [-1, 197, 768]               0\n","          Dropout-35             [-1, 197, 768]               0\n","      ResidualAdd-36             [-1, 197, 768]               0\n","        LayerNorm-37             [-1, 197, 768]           1,536\n","           Linear-38            [-1, 197, 3072]       2,362,368\n","             GELU-39            [-1, 197, 3072]               0\n","          Dropout-40            [-1, 197, 3072]               0\n","           Linear-41             [-1, 197, 768]       2,360,064\n","      ResidualAdd-42             [-1, 197, 768]               0\n","        LayerNorm-43             [-1, 197, 768]           1,536\n","           Linear-44            [-1, 197, 2304]       1,771,776\n","          Dropout-45          [-1, 8, 197, 197]               0\n","           Linear-46             [-1, 197, 768]         590,592\n","MultiHeadAttention-47             [-1, 197, 768]               0\n","          Dropout-48             [-1, 197, 768]               0\n","      ResidualAdd-49             [-1, 197, 768]               0\n","        LayerNorm-50             [-1, 197, 768]           1,536\n","           Linear-51            [-1, 197, 3072]       2,362,368\n","             GELU-52            [-1, 197, 3072]               0\n","          Dropout-53            [-1, 197, 3072]               0\n","           Linear-54             [-1, 197, 768]       2,360,064\n","      ResidualAdd-55             [-1, 197, 768]               0\n","        LayerNorm-56             [-1, 197, 768]           1,536\n","           Linear-57            [-1, 197, 2304]       1,771,776\n","          Dropout-58          [-1, 8, 197, 197]               0\n","           Linear-59             [-1, 197, 768]         590,592\n","MultiHeadAttention-60             [-1, 197, 768]               0\n","          Dropout-61             [-1, 197, 768]               0\n","      ResidualAdd-62             [-1, 197, 768]               0\n","        LayerNorm-63             [-1, 197, 768]           1,536\n","           Linear-64            [-1, 197, 3072]       2,362,368\n","             GELU-65            [-1, 197, 3072]               0\n","          Dropout-66            [-1, 197, 3072]               0\n","           Linear-67             [-1, 197, 768]       2,360,064\n","      ResidualAdd-68             [-1, 197, 768]               0\n","        LayerNorm-69             [-1, 197, 768]           1,536\n","           Linear-70            [-1, 197, 2304]       1,771,776\n","          Dropout-71          [-1, 8, 197, 197]               0\n","           Linear-72             [-1, 197, 768]         590,592\n","MultiHeadAttention-73             [-1, 197, 768]               0\n","          Dropout-74             [-1, 197, 768]               0\n","      ResidualAdd-75             [-1, 197, 768]               0\n","        LayerNorm-76             [-1, 197, 768]           1,536\n","           Linear-77            [-1, 197, 3072]       2,362,368\n","             GELU-78            [-1, 197, 3072]               0\n","          Dropout-79            [-1, 197, 3072]               0\n","           Linear-80             [-1, 197, 768]       2,360,064\n","      ResidualAdd-81             [-1, 197, 768]               0\n","        LayerNorm-82             [-1, 197, 768]           1,536\n","           Linear-83            [-1, 197, 2304]       1,771,776\n","          Dropout-84          [-1, 8, 197, 197]               0\n","           Linear-85             [-1, 197, 768]         590,592\n","MultiHeadAttention-86             [-1, 197, 768]               0\n","          Dropout-87             [-1, 197, 768]               0\n","      ResidualAdd-88             [-1, 197, 768]               0\n","        LayerNorm-89             [-1, 197, 768]           1,536\n","           Linear-90            [-1, 197, 3072]       2,362,368\n","             GELU-91            [-1, 197, 3072]               0\n","          Dropout-92            [-1, 197, 3072]               0\n","           Linear-93             [-1, 197, 768]       2,360,064\n","      ResidualAdd-94             [-1, 197, 768]               0\n","        LayerNorm-95             [-1, 197, 768]           1,536\n","           Linear-96            [-1, 197, 2304]       1,771,776\n","          Dropout-97          [-1, 8, 197, 197]               0\n","           Linear-98             [-1, 197, 768]         590,592\n","MultiHeadAttention-99             [-1, 197, 768]               0\n","         Dropout-100             [-1, 197, 768]               0\n","     ResidualAdd-101             [-1, 197, 768]               0\n","       LayerNorm-102             [-1, 197, 768]           1,536\n","          Linear-103            [-1, 197, 3072]       2,362,368\n","            GELU-104            [-1, 197, 3072]               0\n","         Dropout-105            [-1, 197, 3072]               0\n","          Linear-106             [-1, 197, 768]       2,360,064\n","     ResidualAdd-107             [-1, 197, 768]               0\n","       LayerNorm-108             [-1, 197, 768]           1,536\n","          Linear-109            [-1, 197, 2304]       1,771,776\n","         Dropout-110          [-1, 8, 197, 197]               0\n","          Linear-111             [-1, 197, 768]         590,592\n","MultiHeadAttention-112             [-1, 197, 768]               0\n","         Dropout-113             [-1, 197, 768]               0\n","     ResidualAdd-114             [-1, 197, 768]               0\n","       LayerNorm-115             [-1, 197, 768]           1,536\n","          Linear-116            [-1, 197, 3072]       2,362,368\n","            GELU-117            [-1, 197, 3072]               0\n","         Dropout-118            [-1, 197, 3072]               0\n","          Linear-119             [-1, 197, 768]       2,360,064\n","     ResidualAdd-120             [-1, 197, 768]               0\n","       LayerNorm-121             [-1, 197, 768]           1,536\n","          Linear-122            [-1, 197, 2304]       1,771,776\n","         Dropout-123          [-1, 8, 197, 197]               0\n","          Linear-124             [-1, 197, 768]         590,592\n","MultiHeadAttention-125             [-1, 197, 768]               0\n","         Dropout-126             [-1, 197, 768]               0\n","     ResidualAdd-127             [-1, 197, 768]               0\n","       LayerNorm-128             [-1, 197, 768]           1,536\n","          Linear-129            [-1, 197, 3072]       2,362,368\n","            GELU-130            [-1, 197, 3072]               0\n","         Dropout-131            [-1, 197, 3072]               0\n","          Linear-132             [-1, 197, 768]       2,360,064\n","     ResidualAdd-133             [-1, 197, 768]               0\n","       LayerNorm-134             [-1, 197, 768]           1,536\n","          Linear-135            [-1, 197, 2304]       1,771,776\n","         Dropout-136          [-1, 8, 197, 197]               0\n","          Linear-137             [-1, 197, 768]         590,592\n","MultiHeadAttention-138             [-1, 197, 768]               0\n","         Dropout-139             [-1, 197, 768]               0\n","     ResidualAdd-140             [-1, 197, 768]               0\n","       LayerNorm-141             [-1, 197, 768]           1,536\n","          Linear-142            [-1, 197, 3072]       2,362,368\n","            GELU-143            [-1, 197, 3072]               0\n","         Dropout-144            [-1, 197, 3072]               0\n","          Linear-145             [-1, 197, 768]       2,360,064\n","     ResidualAdd-146             [-1, 197, 768]               0\n","       LayerNorm-147             [-1, 197, 768]           1,536\n","          Linear-148            [-1, 197, 2304]       1,771,776\n","         Dropout-149          [-1, 8, 197, 197]               0\n","          Linear-150             [-1, 197, 768]         590,592\n","MultiHeadAttention-151             [-1, 197, 768]               0\n","         Dropout-152             [-1, 197, 768]               0\n","     ResidualAdd-153             [-1, 197, 768]               0\n","       LayerNorm-154             [-1, 197, 768]           1,536\n","          Linear-155            [-1, 197, 3072]       2,362,368\n","            GELU-156            [-1, 197, 3072]               0\n","         Dropout-157            [-1, 197, 3072]               0\n","          Linear-158             [-1, 197, 768]       2,360,064\n","     ResidualAdd-159             [-1, 197, 768]               0\n","          Reduce-160                  [-1, 768]               0\n","       LayerNorm-161                  [-1, 768]           1,536\n","          Linear-162                   [-1, 10]           7,690\n","================================================================\n","Total params: 85,654,282\n","Trainable params: 85,654,282\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 350.47\n","Params size (MB): 326.75\n","Estimated Total Size (MB): 677.79\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["#ViT 모델 정의\n","model = ViT(n_classes=6)\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"z46V0B6KC0MS","executionInfo":{"status":"ok","timestamp":1700564501611,"user_tz":-540,"elapsed":941,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","num_epochs = 10\n","# 에폭을 반복하면서 학습과 검증을 수행\n","for epoch in range(num_epochs):\n","    i=0\n","    # 학습 단계\n","    for inputs, targets in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs,targets)\n","        loss.backward()\n","        optimizer.step()\n","        print(f'epoch: {epoch}, batch: {i}')\n","        i+=1\n","    # 각 에폭 종료 후 검증 단계\n","    model.eval()  # 모델을 평가 모드로 전환\n","    val_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for val_inputs, val_targets in valid_loader:\n","            val_outputs = model(val_inputs)\n","            val_loss += criterion(val_outputs, val_targets).item()\n","\n","            # 정확도 계산\n","            _, predicted = torch.max(val_outputs, 1)\n","            correct_predictions += (predicted == val_targets).sum().item()\n","            total_samples += val_targets.size(0)\n","\n","    average_val_loss = val_loss / len(valid_loader)\n","    accuracy = (correct_predictions / total_samples) * 100\n","\n","    # 에폭 종료 후 출력\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {average_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n","\n","    checkpoint = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'accuracy':accuracy,\n","    }\n","\n","    # 저장 경로 설정\n","    save_path = '/content/drive/MyDrive/ViT_checkpoint_epoch{}.pt'.format(epoch)\n","\n","    # 딕셔너리를 파일로 저장\n","    torch.save(checkpoint, save_path)\n","\n","\n","    model.train()  # 모델을 학습 모드로 전환"],"metadata":{"id":"UTo7dCV2DCnb","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1700567094009,"user_tz":-540,"elapsed":2592400,"user":{"displayName":"김성아","userId":"17267080986120858203"}},"outputId":"ac52ee88-5cd9-438f-9d4f-9dd812aa69fa"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, batch: 0\n","epoch: 0, batch: 1\n","epoch: 0, batch: 2\n","epoch: 0, batch: 3\n","epoch: 0, batch: 4\n","epoch: 0, batch: 5\n","epoch: 0, batch: 6\n","epoch: 0, batch: 7\n","epoch: 0, batch: 8\n","epoch: 0, batch: 9\n","epoch: 0, batch: 10\n","epoch: 0, batch: 11\n","epoch: 0, batch: 12\n","epoch: 0, batch: 13\n","epoch: 0, batch: 14\n","epoch: 0, batch: 15\n","epoch: 0, batch: 16\n","epoch: 0, batch: 17\n","epoch: 0, batch: 18\n","epoch: 0, batch: 19\n","epoch: 0, batch: 20\n","epoch: 0, batch: 21\n","epoch: 0, batch: 22\n","epoch: 0, batch: 23\n","epoch: 0, batch: 24\n","epoch: 0, batch: 25\n","epoch: 0, batch: 26\n","epoch: 0, batch: 27\n","epoch: 0, batch: 28\n","epoch: 0, batch: 29\n","epoch: 0, batch: 30\n","epoch: 0, batch: 31\n","epoch: 0, batch: 32\n","epoch: 0, batch: 33\n","epoch: 0, batch: 34\n","epoch: 0, batch: 35\n","epoch: 0, batch: 36\n","epoch: 0, batch: 37\n","epoch: 0, batch: 38\n","epoch: 0, batch: 39\n","epoch: 0, batch: 40\n","epoch: 0, batch: 41\n","epoch: 0, batch: 42\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-f3a81dedb009>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch: {epoch}, batch: {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["saved_epoch=0\n","checkpoint = torch.load('/content/drive/MyDrive/ViT_checkpoint_epoch{}.pt'.format(saved_epoch))\n","# 모델과 옵티마이저의 상태 복원\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","#epoch = checkpoint['epoch']\n","accuracy = checkpoint['accuracy']\n","print(accuracy)"],"metadata":{"id":"fvtfRxPp0sCt","executionInfo":{"status":"aborted","timestamp":1700567094009,"user_tz":-540,"elapsed":7,"user":{"displayName":"김성아","userId":"17267080986120858203"}}},"execution_count":null,"outputs":[]}]}